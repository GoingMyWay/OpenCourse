{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def load_pickled_data(pickled_file):\n",
    "    \"\"\"\n",
    "    load picked data\n",
    "    :param pickled_file:\n",
    "    :return: train_data, train_labels, test_data,\n",
    "             test_labels, valid_data, valid_labels\n",
    "    \"\"\"\n",
    "    with open(pickled_file, 'rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        _train_data = save['train_data']\n",
    "        _train_labels = save['train_labels']\n",
    "        _test_data = save['test_data']\n",
    "        _test_labels = save['test_labels']\n",
    "        _valid_data = save['valid_data']\n",
    "        _valid_labels = save['valid_labels']\n",
    "        del save\n",
    "        print(_train_data.shape, _train_labels.shape)\n",
    "        print(_test_data.shape, _test_labels.shape)\n",
    "        print(_valid_data.shape, _valid_labels.shape)\n",
    "    return _train_data, _train_labels, _test_data, _test_labels, _valid_data, _valid_labels\n",
    "\n",
    "\n",
    "def accuracy_func(predicts, labels):\n",
    "    \"\"\"\n",
    "    total accuracy, digit-wise\n",
    "    :param predicts:\n",
    "    :param labels:\n",
    "    :return: float value, precesion\n",
    "    \"\"\"\n",
    "    return 100.0 * np.sum(np.argmax(predicts, 2).T == labels) / predicts.shape[1] / predicts.shape[0]\n",
    "\n",
    "\n",
    "def local_contrast_normalization(input_data, image_shape, threshold=1e-4, radius=7):\n",
    "    \"\"\"\n",
    "    Local Contrast Normalization\n",
    "    :param input_data: input data\n",
    "    :param image_shape: image shape\n",
    "    :param threshold: threshold\n",
    "    :param radius: redius\n",
    "    :return: local contrast normalized input data\n",
    "    \"\"\"\n",
    "    # Gaussian filter\n",
    "    filter_shape = radius, radius, image_shape[3], 1\n",
    "    filters = gaussian_initializer(filter_shape)\n",
    "    input_data = tf.convert_to_tensor(input_data, dtype=tf.float32)\n",
    "    convout = tf.nn.conv2d(input_data, filters, [1, 1, 1, 1], 'SAME')\n",
    "    centered_data = tf.sub(input_data, convout)\n",
    "    denoms = tf.sqrt(tf.nn.conv2d(tf.square(centered_data), filters, [1, 1, 1, 1], 'SAME'))\n",
    "    mean = tf.reduce_mean(denoms)\n",
    "    divisor = tf.maximum(mean, denoms)\n",
    "    # Divisise step\n",
    "    new_data = tf.truediv(centered_data, tf.maximum(divisor, threshold))\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def gaussian_initializer(kernel_shape):\n",
    "    \"\"\"\n",
    "    initialize the kernel weights\n",
    "    :param kernel_shape: kernel shape\n",
    "    :return: tensor\n",
    "    \"\"\"\n",
    "    x = np.zeros(kernel_shape, dtype=float)\n",
    "    mid = np.floor(kernel_shape[0] / 2.)\n",
    "    for kernel_idx in range(0, kernel_shape[2]):\n",
    "        for i in range(0, kernel_shape[0]):\n",
    "            for j in range(0, kernel_shape[1]):\n",
    "                x[i, j, kernel_idx, 0] = gaussian(i - mid, j - mid)\n",
    "    return tf.convert_to_tensor(x / np.sum(x), dtype=tf.float32)\n",
    "\n",
    "\n",
    "def gaussian(x, y, sigma=3.0):\n",
    "    \"\"\"\n",
    "    gaussian function\n",
    "    :param x: x value\n",
    "    :param y: y value\n",
    "    :param sigma: sigma\n",
    "    :return: guassian normalized value\n",
    "    \"\"\"\n",
    "    z = 2 * np.pi * sigma ** 2\n",
    "    return 1. / z * np.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiDigits(object):\n",
    "    \"\"\"\n",
    "    Multi Digits Recognition Model\n",
    "    \"\"\"\n",
    "    def __init__(self, picked_file=None, image_size=32, num_labels=11, num_channels=1,\n",
    "                 batch_size=64, patch_size=5, depth_1=16, depth_2=32, depth_3=64,\n",
    "                 hidden_num=64, num_hidden1=64\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param picked_file:\n",
    "        :param image_size:\n",
    "        :param num_labels:\n",
    "        :param num_channels:\n",
    "        :param batch_size:\n",
    "        :param patch_size:\n",
    "        :param depth_1:\n",
    "        :param depth_2:\n",
    "        :param depth_3:\n",
    "        :param hidden_num:\n",
    "        :param num_hidden1:\n",
    "        \"\"\"\n",
    "        if picked_file is not None:\n",
    "            self.train_data, self.train_labels, self.test_data, \\\n",
    "                self.test_labels, self.valid_data, self.valid_labels = \\\n",
    "                load_pickled_data(picked_file)\n",
    "        self.train_graph = tf.Graph()\n",
    "        self.infer_graph = tf.Graph()\n",
    "        self.image_size = image_size\n",
    "        self.num_labels = num_labels\n",
    "        self.num_channels = num_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.depth_1 = depth_1\n",
    "        self.depth_2 = depth_2\n",
    "        self.depth_3 = depth_3\n",
    "        self.hidden_num = hidden_num\n",
    "        self.num_hidden1 = num_hidden1\n",
    "        self.shape = [batch_size, image_size, image_size, num_channels]\n",
    "        self.saver = None\n",
    "        self.valid_prediction, self.test_prediction = None, None\n",
    "        self.tf_train_dataset = tf.placeholder(tf.float32, shape=self.shape)\n",
    "        self.tf_train_labels = None\n",
    "        self.tf_valid_dataset = None\n",
    "        self.tf_test_dataset = None\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "        self.train_prediction = None\n",
    "        self.save_path = None\n",
    "        self.infer_saver = None\n",
    "        self.is_inited = False\n",
    "        self.conv_layer1_weights = None\n",
    "        self.conv_layer1_biases = None\n",
    "        self.conv_layer2_weights = None\n",
    "        self.conv_layer2_biases = None\n",
    "        self.conv_layer2_biases = None\n",
    "        self.conv_layer3_weights = None\n",
    "        self.conv_layer3_biases = None\n",
    "        self.out_weights_1 = None\n",
    "        self.out_biases_1 = None\n",
    "        self.out_weights_2 = None\n",
    "        self.out_weights_2 = None\n",
    "        self.out_biases_2 = None\n",
    "        self.out_weights_3 = None\n",
    "        self.out_biases_3 = None\n",
    "        self.out_weights_4 = None\n",
    "        self.out_biases_4 = None\n",
    "        self.out_weights_5 = None\n",
    "        self.out_biases_5 = None\n",
    "\n",
    "    def define_graph(self):\n",
    "        with self.train_graph.as_default():\n",
    "            # Input Data.\n",
    "            self.tf_train_dataset = tf.placeholder(tf.float32, shape=self.shape)\n",
    "            self.tf_train_labels = tf.placeholder(tf.int32, shape=(self.batch_size, 6))\n",
    "            self.tf_valid_dataset = tf.constant(self.valid_data)\n",
    "            self.tf_test_dataset = tf.constant(self.test_data)\n",
    "            # init varibales\n",
    "            # Conv Layers\n",
    "            self.conv_layer1_weights = tf.get_variable('c_1_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                       self.num_channels, self.depth_1],\n",
    "                                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.conv_layer1_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_1]), name='c_1_b')\n",
    "            self.conv_layer2_weights = tf.get_variable('c_2_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                       self.depth_1, self.depth_2],\n",
    "                                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.conv_layer2_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_2]), name='c_2_b')\n",
    "            self.conv_layer3_weights = tf.get_variable('c_3_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                       self.depth_2, self.num_hidden1],\n",
    "                                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.conv_layer3_biases = tf.Variable(tf.constant(1.0, shape=[self.num_hidden1]), name='c_3_b')\n",
    "            # Output Layer\n",
    "            self.out_weights_1 = tf.get_variable('o_1', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_1 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_1'))\n",
    "            self.out_weights_2 = tf.get_variable('o_2', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_2 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_2'))\n",
    "            self.out_weights_3 = tf.get_variable('o_3', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_3 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_3'))\n",
    "            self.out_weights_4 = tf.get_variable('o_4', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_4 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_4'))\n",
    "            self.out_weights_5 = tf.get_variable('o_5', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_5 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_5'))\n",
    "            # Training computation.\n",
    "            logits1, logits2, logits3, logits4, logits5 = self.__infer(self.tf_train_dataset, 0.95, self.shape)\n",
    "            self.loss = \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, self.tf_train_labels[:, 1])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits2, self.tf_train_labels[:, 2])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits3, self.tf_train_labels[:, 3])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits4, self.tf_train_labels[:, 4])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits5, self.tf_train_labels[:, 5]))\n",
    "            # Optimizer.\n",
    "            global_step = tf.Variable(0)\n",
    "            learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "            self.optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(self.loss, global_step=global_step)\n",
    "            # Predictions of the training, validation, and test data.\n",
    "            self.train_prediction = tf.pack(list(map(tf.nn.softmax,\n",
    "                                                     self.__infer(self.tf_train_dataset, 1.0, self.shape))))\n",
    "            self.valid_prediction = tf.pack(list(map(tf.nn.softmax,\n",
    "                                                     self.__infer(self.tf_valid_dataset, 1.0, self.shape))))\n",
    "            self.test_prediction = tf.pack(list(map(tf.nn.softmax,\n",
    "                                                    self.__infer(self.tf_test_dataset, 1.0, self.shape))))\n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "    def train_model(self, save_path=None, save=True, epoch=100000):\n",
    "        \"\"\"\n",
    "        训练模型，部署应用的时候不能调用\n",
    "        :param save_path: ckpt数据保存路径\n",
    "        :param save: 是否保存ckpt数据\n",
    "        :param epoch: 训练迭代次数\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        epoch_index = []\n",
    "        losses = []\n",
    "        mini_batch_acc = []\n",
    "        valid_batch_acc = []\n",
    "        epochs = epoch\n",
    "        start_time = time.time()\n",
    "        with tf.Session(graph=self.train_graph) as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('Initialized all variables')\n",
    "            for e in range(epochs):\n",
    "                offset = (e * self.batch_size) % (self.train_labels.shape[0] - self.batch_size)\n",
    "                batch_data = self.train_data[offset:(offset + self.batch_size), :, :, :]\n",
    "                batch_labels = self.train_labels[offset:(offset + self.batch_size), :]\n",
    "                feed_dict = {self.tf_train_dataset: batch_data, self.tf_train_labels: batch_labels}\n",
    "                _, l, predictions = sess.run([self.optimizer, self.loss, self.train_prediction], feed_dict=feed_dict)\n",
    "                if e % 1000 == 0:\n",
    "                    epoch_index.append(e)\n",
    "                    mini_acc = accuracy_func(predictions, batch_labels[:, 1:6])\n",
    "                    mini_batch_acc.append(mini_acc)\n",
    "                    valid_acc = accuracy_func(self.valid_prediction.eval(), self.valid_labels[:, 1:6])\n",
    "                    valid_batch_acc.append(valid_acc)\n",
    "                    losses.append(l)\n",
    "                    print('Minibatch loss at step %d: %f' % (e, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % mini_acc)\n",
    "                    print('Validation accuracy: %.1f%%' % valid_acc)\n",
    "            print('Test accuracy: %.1f%%' % accuracy_func(self.test_prediction.eval(), self.test_labels[:, 1:6]))\n",
    "            if save:\n",
    "                self.save_path = self.saver.save(sess, save_path)\n",
    "                print(\"Model saved in file: %s\" % self.save_path)\n",
    "            end_time = time.time()\n",
    "            print('train time: %s' % (end_time - start_time))\n",
    "        return epoch_index, losses, mini_batch_acc, valid_batch_acc\n",
    "\n",
    "    def infer_data(self, input_data, ckpt_path):\n",
    "        \"\"\"\n",
    "        infer input data\n",
    "        :param input_data: input a instance\n",
    "        :param ckpt_path: path to the ckpt file\n",
    "        :return: return result\n",
    "        \"\"\"\n",
    "        infer_graph = tf.Graph()\n",
    "        with infer_graph.as_default():\n",
    "            # Input Data.\n",
    "            tf_infer_data = tf.placeholder(tf.float32, shape=(1, 32, 32, 1))\n",
    "\n",
    "            # init varibales\n",
    "            conv_layer1_weights = tf.get_variable('c_1_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.num_channels, self.depth_1],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer1_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_1]), name='c_1_b')\n",
    "            conv_layer2_weights = tf.get_variable('c_2_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.depth_1, self.depth_2],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer2_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_2]), name='c_2_b')\n",
    "            conv_layer3_weights = tf.get_variable('c_3_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.depth_2, self.num_hidden1],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer3_biases = tf.Variable(tf.constant(1.0, shape=[self.num_hidden1]), name='c_3_b')\n",
    "            # Output Layer\n",
    "            out_weights_1 = tf.get_variable('o_1', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_1 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_1'))\n",
    "            out_weights_2 = tf.get_variable('o_2', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_2 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_2'))\n",
    "            out_weights_3 = tf.get_variable('o_3', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_3 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_3'))\n",
    "            out_weights_4 = tf.get_variable('o_4', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_4 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_4'))\n",
    "            out_weights_5 = tf.get_variable('o_5', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_5 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_5'))\n",
    "\n",
    "            def infer(data, keep_prob, d_shape):\n",
    "                # conv layer\n",
    "                lcn = local_contrast_normalization(data, d_shape)\n",
    "                conv_1 = tf.nn.conv2d(lcn, conv_layer1_weights, [1, 1, 1, 1], 'VALID', name='c_1')\n",
    "                conv_1 = tf.nn.relu(conv_1 + conv_layer1_biases)\n",
    "                conv_1 = tf.nn.local_response_normalization(conv_1)\n",
    "                pool_1 = tf.nn.max_pool(conv_1, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_1')\n",
    "                conv_2 = tf.nn.conv2d(pool_1, conv_layer2_weights, [1, 1, 1, 1], padding='VALID', name='c_2')\n",
    "                conv_2 = tf.nn.relu(conv_2 + conv_layer2_biases)\n",
    "                conv_2 = tf.nn.local_response_normalization(conv_2)\n",
    "                pool_2 = tf.nn.max_pool(conv_2, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_2_')\n",
    "                conv_3 = tf.nn.conv2d(pool_2, conv_layer3_weights, [1, 1, 1, 1], padding='VALID', name='c_3')\n",
    "                conv_3 = tf.nn.relu(conv_3 + conv_layer3_biases)\n",
    "                conv_3 = tf.nn.dropout(conv_3, keep_prob)\n",
    "                shapes = conv_3.get_shape().as_list()\n",
    "                hidden = tf.reshape(conv_3, [shapes[0], shapes[1] * shapes[2] * shapes[3]])\n",
    "                # fc layer\n",
    "                logits_1 = tf.matmul(hidden, out_weights_1) + out_biases_1\n",
    "                logits_2 = tf.matmul(hidden, out_weights_2) + out_biases_2\n",
    "                logits_3 = tf.matmul(hidden, out_weights_3) + out_biases_3\n",
    "                logits_4 = tf.matmul(hidden, out_weights_4) + out_biases_4\n",
    "                logits_5 = tf.matmul(hidden, out_weights_5) + out_biases_5\n",
    "                return logits_1, logits_2, logits_3, logits_4, logits_5\n",
    "\n",
    "            # Predictions\n",
    "            infer_predict = tf.pack(list(map(tf.nn.softmax, infer(tf_infer_data, 1.0, self.shape))))\n",
    "            prediction = tf.transpose(tf.argmax(infer_predict, 2))\n",
    "            self.infer_saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session(graph=infer_graph) as session:\n",
    "            self.infer_saver.restore(session, save_path=ckpt_path)\n",
    "            input_prediction, infer_prediction = session.run([prediction, infer_predict],\n",
    "                                                             feed_dict={tf_infer_data: input_data})\n",
    "            return input_prediction\n",
    "\n",
    "    def __infer(self, data, keep_prob, d_shape):\n",
    "        # conv layer\n",
    "        lcn = local_contrast_normalization(data, d_shape)\n",
    "        conv_1 = tf.nn.conv2d(lcn, self.conv_layer1_weights, [1, 1, 1, 1], 'VALID', name='c_1')\n",
    "        conv_1 = tf.nn.relu(conv_1 + self.conv_layer1_biases)\n",
    "        lrn = tf.nn.local_response_normalization(conv_1)\n",
    "        pool_1 = tf.nn.max_pool(lrn, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_1')\n",
    "        conv_2 = tf.nn.conv2d(pool_1, self.conv_layer2_weights, [1, 1, 1, 1], padding='VALID', name='c_2')\n",
    "        conv_2 = tf.nn.relu(conv_2 + self.conv_layer2_biases)\n",
    "        lrn = tf.nn.local_response_normalization(conv_2)\n",
    "        pool_2 = tf.nn.max_pool(lrn, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_2_')\n",
    "        conv_3 = tf.nn.conv2d(pool_2, self.conv_layer3_weights, [1, 1, 1, 1], padding='VALID', name='c_3')\n",
    "        conv_3 = tf.nn.relu(conv_3 + self.conv_layer3_biases)\n",
    "        conv_3 = tf.nn.dropout(conv_3, keep_prob)\n",
    "        shapes = conv_3.get_shape().as_list()\n",
    "        hidden = tf.reshape(conv_3, [shapes[0], shapes[1] * shapes[2] * shapes[3]])\n",
    "        # fc layer\n",
    "        logits_1 = tf.matmul(hidden, self.out_weights_1) + self.out_biases_1\n",
    "        logits_2 = tf.matmul(hidden, self.out_weights_2) + self.out_biases_2\n",
    "        logits_3 = tf.matmul(hidden, self.out_weights_3) + self.out_biases_3\n",
    "        logits_4 = tf.matmul(hidden, self.out_weights_4) + self.out_biases_4\n",
    "        logits_5 = tf.matmul(hidden, self.out_weights_5) + self.out_biases_5\n",
    "        return logits_1, logits_2, logits_3, logits_4, logits_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((230070, 32, 32, 1), (230070, 6))\n",
      "((13068, 32, 32, 1), (13068, 6))\n",
      "((5684, 32, 32, 1), (5684, 6))\n"
     ]
    }
   ],
   "source": [
    "train_model = MultiDigits('SVHN.pickle') \n",
    "train_model.define_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized all variables\n",
      "Minibatch loss at step 0: 17.476204\n",
      "Minibatch accuracy: 5.3%\n",
      "Validation accuracy: 43.8%\n",
      "Minibatch loss at step 1000: 3.540220\n",
      "Minibatch accuracy: 79.4%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 2000: 2.048233\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 3000: 1.777309\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 4000: 2.447232\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 5000: 1.365698\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 6000: 1.219130\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 7000: 2.277873\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8000: 1.150327\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9000: 1.522681\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 10000: 1.192972\n",
      "Minibatch accuracy: 92.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 11000: 1.394461\n",
      "Minibatch accuracy: 92.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 12000: 2.197570\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 13000: 1.442187\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 14000: 1.953730\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 15000: 1.006747\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 16000: 1.431585\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 17000: 1.655029\n",
      "Minibatch accuracy: 93.1%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 18000: 1.403563\n",
      "Minibatch accuracy: 93.1%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 19000: 1.146169\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 20000: 1.418871\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 21000: 0.737013\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 22000: 1.030006\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 23000: 1.153932\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 24000: 0.909201\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 25000: 1.706597\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 26000: 1.552922\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 27000: 0.756770\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 28000: 1.074812\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 29000: 0.931204\n",
      "Minibatch accuracy: 93.1%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 30000: 1.026205\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 31000: 1.300133\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 32000: 1.059006\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 33000: 0.793552\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 34000: 0.961200\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 35000: 1.114156\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 36000: 0.802135\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 37000: 0.828002\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 38000: 0.804800\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 39000: 1.599764\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 40000: 0.889095\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 41000: 0.737869\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 42000: 0.893463\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 43000: 1.482470\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 44000: 1.005072\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 45000: 0.958800\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 46000: 0.891084\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 47000: 1.186259\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 48000: 1.079028\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 49000: 0.838592\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 50000: 1.475133\n",
      "Minibatch accuracy: 92.8%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 51000: 0.791847\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 52000: 0.781317\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 53000: 1.066313\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 54000: 0.649781\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 55000: 1.000316\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 56000: 1.316519\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 57000: 0.956032\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 58000: 0.990633\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 59000: 0.699405\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 60000: 1.021618\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 61000: 0.955613\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 62000: 1.013686\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 63000: 1.180671\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 64000: 0.485771\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 65000: 0.866799\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 66000: 0.888513\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 67000: 0.990653\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 68000: 0.943704\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 69000: 1.562682\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 70000: 0.848867\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 71000: 0.875218\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 72000: 0.883906\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 73000: 0.838155\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 74000: 1.040615\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 75000: 0.998699\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 76000: 0.583195\n",
      "Minibatch accuracy: 98.1%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 77000: 0.908095\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 78000: 1.381061\n",
      "Minibatch accuracy: 93.1%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 79000: 1.228990\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 80000: 0.687190\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 81000: 0.753505\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 82000: 0.965143\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 83000: 0.949375\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 84000: 0.926841\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 85000: 1.003615\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 86000: 0.987066\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 87000: 0.767150\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 88000: 0.990563\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 89000: 0.548475\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 90000: 0.693282\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 91000: 0.502818\n",
      "Minibatch accuracy: 97.2%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 92000: 0.826773\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 93000: 1.295028\n",
      "Minibatch accuracy: 92.8%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 94000: 0.510350\n",
      "Minibatch accuracy: 99.1%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 95000: 1.027959\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 96000: 0.948648\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 97000: 0.806190\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 98000: 1.140349\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 99000: 0.701851\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 100000: 0.436926\n",
      "Minibatch accuracy: 97.8%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 101000: 0.726309\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 102000: 0.696930\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 103000: 0.806001\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 104000: 1.446046\n",
      "Minibatch accuracy: 93.1%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 105000: 0.674124\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 106000: 0.938594\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 107000: 0.952346\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 108000: 0.659406\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 109000: 0.765276\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 110000: 0.635362\n",
      "Minibatch accuracy: 97.2%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 111000: 1.195642\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 112000: 0.802408\n",
      "Minibatch accuracy: 97.2%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 113000: 0.939947\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 114000: 1.074106\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 115000: 1.776801\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 116000: 0.867601\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 117000: 0.979005\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 118000: 0.657508\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 119000: 0.811285\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 120000: 0.757143\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 121000: 0.663649\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 122000: 1.569659\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 123000: 0.530959\n",
      "Minibatch accuracy: 97.8%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 124000: 0.513122\n",
      "Minibatch accuracy: 97.2%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 125000: 1.325053\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 126000: 0.761840\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 127000: 0.868740\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 128000: 1.080167\n",
      "Minibatch accuracy: 93.1%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 129000: 0.776788\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 130000: 0.996932\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 131000: 0.663094\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 132000: 0.914129\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 133000: 0.907818\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 134000: 0.982202\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 135000: 0.916811\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 136000: 0.661386\n",
      "Minibatch accuracy: 97.2%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 137000: 0.835155\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 138000: 0.738800\n",
      "Minibatch accuracy: 97.2%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 139000: 0.623607\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 140000: 0.839310\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 141000: 0.740683\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 142000: 0.961134\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 143000: 0.732926\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 144000: 0.532083\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 145000: 0.590514\n",
      "Minibatch accuracy: 97.2%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 146000: 0.742729\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 147000: 1.261811\n",
      "Minibatch accuracy: 93.1%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 148000: 0.891678\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 149000: 0.669202\n",
      "Minibatch accuracy: 97.2%\n",
      "Validation accuracy: 93.4%\n",
      "Test accuracy: 93.5%\n",
      "Model saved in file: ckpt_data/SVHN.ckpt\n",
      "train time: 1932.23242497\n"
     ]
    }
   ],
   "source": [
    "epoch_index, losses, mini_batch_acc, valid_batch_acc = \\\n",
    "    train_model.train_model(save_path='ckpt_data/SVHN.ckpt', save=True, epoch=150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexes = [6, 8, 12, 13, 115, 120, 121]\n",
    "input_datas, input_labels = train_model.test_data[indexes, ...], train_model.test_labels[indexes, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 32, 1), array([ 3,  1,  8,  3, 10, 10]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_datas[0].shape, input_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = lambda a,i : int(''.join(map(str, a[i:a.index(10)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('predict', 'real')\n",
      "(183, 183)\n",
      "(144, 144)\n",
      "(19, 13)\n",
      "(25, 25)\n",
      "(20, 20)\n",
      "(22, 22)\n",
      "(9, 9)\n"
     ]
    }
   ],
   "source": [
    "train_model.save_path = \"ckpt_data/SVHN.ckpt\"\n",
    "predicts = []\n",
    "print(('predict', 'real'))\n",
    "for i, input_data in enumerate(input_datas):\n",
    "    print(f(train_model.infer_data(input_data.reshape((1, 32, 32, 1)), ckpt_path=\"ckpt_data/SVHN.ckpt\")[0].tolist(), 0), \n",
    "          f(input_labels[i].tolist(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "---\n",
    "\n",
    "论文[Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks](https://arxiv.org/abs/1312.6082)的convolutional neural network的实现，网络的结构与论文一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((230070, 32, 32, 1), (230070, 6))\n",
      "((13068, 32, 32, 1), (13068, 6))\n",
      "((5684, 32, 32, 1), (5684, 6))\n"
     ]
    }
   ],
   "source": [
    "with open('SVHN.pickle', 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_data = save['train_data']\n",
    "    train_labels = save['train_labels']\n",
    "    test_data = save['test_data']\n",
    "    test_labels = save['test_labels']\n",
    "    valid_data = save['valid_data']\n",
    "    valid_labels = save['valid_labels']\n",
    "    del save\n",
    "    print(train_data.shape, train_labels.shape)\n",
    "    print(test_data.shape, test_labels.shape)\n",
    "    print(valid_data.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  1,  3, 10, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[100,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建网络\n",
    "---\n",
    "\n",
    "网络结构\n",
    "\n",
    "8 ConvNets Layers\n",
    "2 FC hidden layers\n",
    "\n",
    "##### ConvNets Layers\n",
    "\n",
    "* units(means depth) in each layer\n",
    "\n",
    "    [48, 64, 128, 160, 192, 192, 192, 192]\n",
    "\n",
    "* neurons\n",
    "  * layer 1: maxout\n",
    "  * others: ReLU\n",
    "\n",
    "* each layer includes max pooling and subtrative normalization\n",
    "  * max pooling: 2x2 window with 2 and 1 at each layer\n",
    "  * subtrative normalization: 3x3 window, perserves represetation size\n",
    "\n",
    "* conv kernels\n",
    "  * size: 5x5\n",
    "  * padding: zero padding, preserve representation size\n",
    "  * stride: 1\n",
    "  \n",
    "\n",
    "##### FC hidden layers\n",
    "\n",
    "* units in each layer\n",
    "\n",
    "    [3072, 3072]\n",
    "\n",
    "* each layer with drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define settings of the CNN\n",
    "\n",
    "# depth of each conv layer\n",
    "depth_1, depth_2, depth_3, depth_4, depth_5, depth_6, depth_7, depth_8 = [48, 64, 128, 160, 192, 192, 192, 192]\n",
    "\n",
    "batch_size = 128\n",
    "image_size = 32\n",
    "pooling_width = 2\n",
    "sub_norm_width = 3\n",
    "patch_size = 5\n",
    "stride = 1\n",
    "num_channels = 1\n",
    "num_lables = 11\n",
    "\n",
    "num_hidden_neuron = 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 192 and 49152 for 'MatMul' (op: 'MatMul') with input shapes: [128,192], [49152,3072].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-01ac2c4ed2f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogit_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mlogit_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m        \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m        \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m        \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m        \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m        \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-01ac2c4ed2f9>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(data, keep_prob)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool_8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mreshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mhidden_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc_weights_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfc_biases_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mhidden_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mhidden_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc_weights_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfc_biases_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1729\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   1440\u001b[0m   \"\"\"\n\u001b[1;32m   1441\u001b[0m   result = _op_def_lib.apply_op(\"MatMul\", a=a, b=b, transpose_a=transpose_a,\n\u001b[0;32m-> 1442\u001b[0;31m                                 transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1443\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    757\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    758\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2240\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2242\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1615\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 192 and 49152 for 'MatMul' (op: 'MatMul') with input shapes: [128,192], [49152,3072]."
     ]
    }
   ],
   "source": [
    "# create CNN graph and define weights, biases, and other varible etc.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # input data\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, 6))\n",
    "    tf_test_data = tf.constant(test_data)\n",
    "    tf_test_labls = tf.constant(test_labels)\n",
    "    tf_valid_data = tf.constant(valid_data)\n",
    "    tf_valid_labels = tf.constant(valid_labels)\n",
    "    \n",
    "    # define weights and biases\n",
    "    # conv layer weights and biases\n",
    "    conv_weights_1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_1], stddev=0.1), name='c_w_1')\n",
    "    conv_biases_1 = tf.Variable(tf.zeros(shape=[depth_1]), name='c_b_1')\n",
    "    \n",
    "    conv_weights_2 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_1, depth_2], stddev=0.1), name='c_w_2')\n",
    "    conv_biases_2 = tf.Variable(tf.zeros(shape=[depth_2]), name='c_b_2')\n",
    "    \n",
    "    conv_weights_3 = tf.Variable(tf.truncated_normal(shape=[patch_size, patch_size, depth_2, depth_3], stddev=0.1), name='c_w_3')\n",
    "    conv_biases_3 = tf.Variable(tf.zeros(shape=[depth_3]), name='c_b_3')\n",
    "    \n",
    "    conv_weights_4 = tf.Variable(tf.truncated_normal(shape=[patch_size, patch_size, depth_3, depth_4], stddev=0.1), name='c_w_4')\n",
    "    conv_biases_4 = tf.Variable(tf.zeros(shape=[depth_4]), name='c_b_4')\n",
    "    \n",
    "    conv_weights_5 = tf.Variable(tf.truncated_normal(shape=[patch_size, patch_size, depth_4, depth_5], stddev=0.1), name='c_w_5')\n",
    "    conv_biases_5 = tf.Variable(tf.zeros(shape=[depth_5]), name='c_b_5')\n",
    "    \n",
    "    conv_weights_6 = tf.Variable(tf.truncated_normal(shape=[patch_size, patch_size, depth_5, depth_6], stddev=0.1), name='c_w_6')\n",
    "    conv_biases_6 = tf.Variable(tf.zeros(shape=[depth_6]), name='c_b_6')\n",
    "    \n",
    "    conv_weights_7 = tf.Variable(tf.truncated_normal(shape=[patch_size, patch_size, depth_6, depth_7], stddev=0.1), name='c_w_7')\n",
    "    conv_biases_7 = tf.Variable(tf.zeros(shape=[depth_7]), name='c_b_7')\n",
    "    \n",
    "    conv_weights_8 = tf.Variable(tf.truncated_normal(shape=[patch_size, patch_size, depth_7, depth_8], stddev=0.1), name='c_w_8')\n",
    "    conv_biases_8 = tf.Variable(tf.zeros(shape=[depth_8]), name='c_b_8')\n",
    "    # full connected layer weights and biases\n",
    "    fc_weights_1 = tf.Variable(tf.truncated_normal([16 * 16 * 192, num_hidden_neuron], stddev=0.1), name='f_w_1') \n",
    "    fc_biases_1 = tf.Variable(tf.zeros([num_hidden_neuron]), name='f_b_1')\n",
    "    fc_weights_2 = tf.Variable(tf.truncated_normal([num_hidden_neuron, num_hidden_neuron], stddev=0.1), name='f_w_2') \n",
    "    fc_biases_2 = tf.Variable(tf.zeros([num_hidden_neuron]), name='f_b_2')\n",
    "    \n",
    "    # output\n",
    "    fc_weights_out_length = tf.get_variable('f_w_o_len', shape=[num_hidden_neuron, 6], dtype=tf.float32, \n",
    "                                            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    fc_biases_out_length = tf.Variable(tf.zeros([6]), name='f_b_o_len')\n",
    "    \n",
    "    fc_weights_out_1 = tf.get_variable('f_w_o_1', shape=[num_hidden_neuron, num_lables], dtype=tf.float32, \n",
    "                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    fc_biases_out_1 = tf.Variable(tf.zeros([num_lables]), name='f_b_o_1')\n",
    "    \n",
    "    fc_weights_out_2 = tf.get_variable('f_w_o_2', shape=[num_hidden_neuron, num_lables], dtype=tf.float32, \n",
    "                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    fc_biases_out_2 = tf.Variable(tf.zeros([num_lables]), name='f_b_o_2')\n",
    "    \n",
    "    fc_weights_out_3 = tf.get_variable('f_w_o_3', shape=[num_hidden_neuron, num_lables], dtype=tf.float32, \n",
    "                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    fc_biases_out_3 = tf.Variable(tf.zeros([num_lables]), name='f_b_o_3')\n",
    "    \n",
    "    fc_weights_out_4 = tf.get_variable('f_w_o_4', shape=[num_hidden_neuron, num_lables], dtype=tf.float32, \n",
    "                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    fc_biases_out_4 = tf.Variable(tf.zeros([num_lables]), name='f_b_o_4')\n",
    "    \n",
    "    fc_weights_out_5 = tf.get_variable('f_w_o_5', shape=[num_hidden_neuron, num_lables], dtype=tf.float32, \n",
    "                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    fc_biases_out_5 = tf.Variable(tf.zeros([num_lables]), name='f_b_o_5')\n",
    "    \n",
    "    def model(data, keep_prob):\n",
    "        # conv layers\n",
    "        conv_1 = tf.nn.conv2d(input=data, filter=conv_weights_1, strides=[1, 1, 1, 1], \n",
    "                              padding='SAME', name='conv_1') + conv_biases_1\n",
    "        conv_1 = tf.nn.relu(conv_1, name='conv_1_relu')\n",
    "        pool_1 = tf.nn.max_pool(conv_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_1')\n",
    "        \n",
    "        conv_2 = tf.nn.conv2d(input=pool_1, filter=conv_weights_2, strides=[1, 1, 1, 1], \n",
    "                              padding='SAME', name='conv_2') + conv_biases_2\n",
    "        conv_2 = tf.nn.relu(conv_2, name='conv_2_relu')\n",
    "        pool_2 = tf.nn.max_pool(conv_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_2')\n",
    "        \n",
    "        conv_3 = tf.nn.conv2d(input=pool_2, filter=conv_weights_3, strides=[1, 1, 1, 1], \n",
    "                              padding='SAME', name='conv_3') + conv_biases_3\n",
    "        conv_3 = tf.nn.relu(conv_3, name='conv_3_relu')\n",
    "        pool_3 = tf.nn.max_pool(conv_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_3')\n",
    "        \n",
    "        conv_4 = tf.nn.conv2d(input=pool_3, filter=conv_weights_4, strides=[1, 1, 1, 1], \n",
    "                              padding='SAME', name='conv_4') + conv_biases_4\n",
    "        conv_4 = tf.nn.relu(conv_4, name='conv_4_relu')\n",
    "        pool_4 = tf.nn.max_pool(conv_4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_4')\n",
    "        \n",
    "        conv_5 = tf.nn.conv2d(input=pool_4, filter=conv_weights_5, strides=[1, 1, 1, 1], \n",
    "                              padding='SAME', name='conv_5') + conv_biases_5\n",
    "        conv_5 = tf.nn.relu(conv_5, name='conv_5_relu')\n",
    "        pool_5 = tf.nn.max_pool(conv_5, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_5')\n",
    "        \n",
    "        conv_6 = tf.nn.conv2d(input=pool_5, filter=conv_weights_6, strides=[1, 1, 1, 1], \n",
    "                              padding='SAME', name='conv_6') + conv_biases_6\n",
    "        conv_6 = tf.nn.relu(conv_6, name='conv_6_relu')\n",
    "        pool_6 = tf.nn.max_pool(conv_6, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_6')\n",
    "        \n",
    "        conv_7 = tf.nn.conv2d(input=pool_6, filter=conv_weights_7, strides=[1, 1, 1, 1], \n",
    "                              padding='SAME', name='conv_7') + conv_biases_7\n",
    "        conv_7 = tf.nn.relu(conv_7, name='conv_7_relu')\n",
    "        pool_7 = tf.nn.max_pool(conv_7, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_7')\n",
    "        \n",
    "        conv_8 = tf.nn.conv2d(input=pool_7, filter=conv_weights_8, strides=[1, 1, 1, 1], \n",
    "                              padding='SAME', name='conv_8') + conv_biases_8\n",
    "        conv_8 = tf.nn.relu(conv_8, name='conv_8_relu')\n",
    "        pool_8 = tf.nn.max_pool(conv_8, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_8')\n",
    "        # FC layers\n",
    "        shapes = pool_8.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool_8, [shapes[0], shapes[1] * shapes[2]*shapes[3]])\n",
    "        hidden_1 = tf.nn.relu(tf.matmul(reshape, fc_weights_1) + fc_biases_1)\n",
    "        hidden_1 = tf.nn.dropout(hidden_1, keep_prob)\n",
    "        hidden_2 = tf.nn.relu(tf.matmul(hidden_1, fc_weights_2) + fc_biases_2)\n",
    "        hidden_2 = tf.nn.dropout(hidden_2, keep_prob)\n",
    "        \n",
    "        # six output layers\n",
    "        logit_len = tf.matmul(hidden_2, fc_weights_out_length) + fc_biases_out_length\n",
    "        logit_1 = tf.matmul(hidden_2, fc_weights_out_1) + fc_biases_out_1\n",
    "        logit_2 = tf.matmul(hidden_2, fc_weights_out_2) + fc_biases_out_2\n",
    "        logit_3 = tf.matmul(hidden_2, fc_weights_out_3) + fc_biases_out_3\n",
    "        logit_4 = tf.matmul(hidden_2, fc_weights_out_4) + fc_biases_out_4\n",
    "        logit_5 = tf.matmul(hidden_2, fc_weights_out_5) + fc_biases_out_5\n",
    "        return logit_len, logit_1, logit_2, logit_3, logit_4, logit_5\n",
    "        \n",
    "    logit_len, logit_1, logit_2, logit_3, logit_4, logit_5 = model(tf_train_data, 0.8)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logit_len, tf_train_labels[:, 0])) +\\\n",
    "        tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logit_1, tf_train_labels[:, 1])) +\\\n",
    "        tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logit_2, tf_train_labels[:, 2])) +\\\n",
    "        tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logit_3, tf_train_labels[:, 3])) +\\\n",
    "        tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logit_4, tf_train_labels[:, 4])) +\\\n",
    "        tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logit_5, tf_train_labels[:, 5]))\n",
    "        \n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.2, global_step, 10000, 0.95)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    train_prediction = tf.pack(list(map(tf.nn.softmax, model(tf_train_data, 1.0, shape))))\n",
    "    valid_prediction = tf.pack(list(map(tf.nn.softmax, model(tf_valid_data, 1.0, shape))))\n",
    "    test_prediction = tf.pack(list(map(tf.nn.softmax, model(tf_test_data, 1.0, shape))))\n",
    "    \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

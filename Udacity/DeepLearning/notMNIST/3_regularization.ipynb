{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))  / predictions.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I copied some code from assignment 2 that complished yesterday\n",
    "\n",
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "lambda_term = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size ** 2, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) +\n",
    "                          lambda_term * tf.nn.l2_loss(weights) +\n",
    "                          lambda_term * tf.nn.l2_loss(biases))\n",
    "    \n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    # Set small learning rate \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    # train_prediction = tf.nn.softmax(logits) # logits is equal to `tf.matmul(tf_train_dataset, weights) + biases`\n",
    "    train_prediction = tf.nn.softmax(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run this compute and iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-818829af90ce>:12 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Loss at step 0: 50.904644\n",
      "Training accuracy: 7.4%\n",
      "Validation accuracy: 8.7%\n",
      "Loss at step 100: 11.921806\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 72.0%\n",
      "Loss at step 200: 4.500904\n",
      "Training accuracy: 79.7%\n",
      "Validation accuracy: 76.9%\n",
      "Loss at step 300: 1.977441\n",
      "Training accuracy: 82.8%\n",
      "Validation accuracy: 80.0%\n",
      "Loss at step 400: 1.121218\n",
      "Training accuracy: 84.3%\n",
      "Validation accuracy: 81.3%\n",
      "Loss at step 500: 0.827709\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 81.7%\n",
      "Loss at step 600: 0.725956\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 81.8%\n",
      "Loss at step 700: 0.690377\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 81.8%\n",
      "Loss at step 800: 0.677858\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "# I copied some code from assignment 2 that complished yesterday\n",
    "\n",
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, and random.\n",
    "    # tf.global_variables_initializer().run()\n",
    "    tf.initialize_all_variables().run() \n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, train_labels[:train_subset, :]))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Connected Neural Network with one hidden-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ReLU neuron\n",
    "\n",
    "# param\n",
    "training_epochs = 30\n",
    "batch_size = 521\n",
    "display_step = 1\n",
    "n_input = 784 # img shape: 28*28\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# hyper-parameter\n",
    "n_hidden_1 = 256 \n",
    "learning_rate = 0.05\n",
    "lambda_term = 0.01\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # init weights\n",
    "    weights_hiden =  tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=1.0/np.sqrt(n_input)))\n",
    "    weights_out = tf.Variable(tf.random_normal([n_hidden_1, n_classes], stddev=1.0/np.sqrt(n_hidden_1)))\n",
    "    \n",
    "    biases_hidden = tf.Variable(tf.random_normal([n_hidden_1]))\n",
    "    biases_out = tf.Variable(tf.random_normal([n_classes]))\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "    \n",
    "    def model(x, weights_hiden, weights_out, biases_hidden, biases_out):\n",
    "        # hidden layer with RELU activation\n",
    "        layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights_hiden), biases_hidden))\n",
    "        # output layer with linear activation\n",
    "        out_layer = tf.matmul(layer_1, weights_out) + biases_out\n",
    "        return out_layer\n",
    "    \n",
    "    # Construct model\n",
    "    pred = model(x, weights_hiden, weights_out, biases_hidden, biases_out)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y) +\n",
    "                          lambda_term * tf.nn.l2_loss(weights_hiden) + \n",
    "                          lambda_term * tf.nn.l2_loss(weights_out) +\n",
    "                          lambda_term * tf.nn.l2_loss(biases_hidden) + \n",
    "                          lambda_term * tf.nn.l2_loss(biases_out))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-4de3da2b1ba3>:3 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Epoch: 0001 cost= 5.355447769\n",
      "Epoch: 0002 cost= 4.941251755\n",
      "Epoch: 0003 cost= 4.762169838\n",
      "Epoch: 0004 cost= 4.625680923\n",
      "Epoch: 0005 cost= 4.588827133\n",
      "Epoch: 0006 cost= 4.479930401\n",
      "Epoch: 0007 cost= 4.449034691\n",
      "Epoch: 0008 cost= 4.396099091\n",
      "Epoch: 0009 cost= 4.367498398\n",
      "Epoch: 0010 cost= 4.298876286\n",
      "Epoch: 0011 cost= 4.224603176\n",
      "Epoch: 0012 cost= 4.145647049\n",
      "Epoch: 0013 cost= 4.162161350\n",
      "Epoch: 0014 cost= 4.128488541\n",
      "Epoch: 0015 cost= 4.086365223\n",
      "Epoch: 0016 cost= 4.037142277\n",
      "Epoch: 0017 cost= 4.013607979\n",
      "Epoch: 0018 cost= 3.960484505\n",
      "Epoch: 0019 cost= 3.921268940\n",
      "Epoch: 0020 cost= 3.902770758\n",
      "Epoch: 0021 cost= 3.899853706\n",
      "Epoch: 0022 cost= 3.918957949\n",
      "Epoch: 0023 cost= 3.811633110\n",
      "Epoch: 0024 cost= 3.782674551\n",
      "Epoch: 0025 cost= 3.802799940\n",
      "Epoch: 0026 cost= 3.741531372\n",
      "Epoch: 0027 cost= 3.725215912\n",
      "Epoch: 0028 cost= 3.685925484\n",
      "Epoch: 0029 cost= 3.651138544\n",
      "Epoch: 0030 cost= 3.655175209\n",
      "Optimization Finished!\n",
      "Test data accuracy: 0.8083\n",
      "Valid data accuracy: 0.7395\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        offset = (epoch * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_x = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_y = train_labels[offset:(offset + batch_size), :]\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "        # Compute average loss\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Test data accuracy:\", accuracy.eval({x: test_dataset, y: test_labels}))\n",
    "    print(\"Valid data accuracy:\", accuracy.eval({x: valid_dataset, y: valid_labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here just show the result of Full Connected Neural Network with one-hidden layer with ReLU neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I just copy all the code above\n",
    "\n",
    "# ReLU neuron\n",
    "\n",
    "# param\n",
    "training_epochs = 30\n",
    "batch_size = 1 #  Here\n",
    "display_step = 1\n",
    "n_input = 784 # img shape: 28*28\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# hyper-parameter\n",
    "n_hidden_1 = 256 \n",
    "learning_rate = 0.05\n",
    "lambda_term = 0.01\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # init weights\n",
    "    weights_hiden =  tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=1.0/np.sqrt(n_input)))\n",
    "    weights_out = tf.Variable(tf.random_normal([n_hidden_1, n_classes], stddev=1.0/np.sqrt(n_hidden_1)))\n",
    "    \n",
    "    biases_hidden = tf.Variable(tf.random_normal([n_hidden_1]))\n",
    "    biases_out = tf.Variable(tf.random_normal([n_classes]))\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "    \n",
    "    def model(x, weights_hiden, weights_out, biases_hidden, biases_out):\n",
    "        # hidden layer with RELU activation\n",
    "        layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights_hiden), biases_hidden))\n",
    "        # output layer with linear activation\n",
    "        out_layer = tf.matmul(layer_1, weights_out) + biases_out\n",
    "        return out_layer\n",
    "    \n",
    "    # Construct model\n",
    "    pred = model(x, weights_hiden, weights_out, biases_hidden, biases_out)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y) +\n",
    "                          lambda_term * tf.nn.l2_loss(weights_hiden) + \n",
    "                          lambda_term * tf.nn.l2_loss(weights_out) +\n",
    "                          lambda_term * tf.nn.l2_loss(biases_hidden) + \n",
    "                          lambda_term * tf.nn.l2_loss(biases_out))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-4de3da2b1ba3>:3 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Epoch: 0001 cost= 3.243125677\n",
      "Epoch: 0002 cost= 7.185295582\n",
      "Epoch: 0003 cost= 9.017312050\n",
      "Epoch: 0004 cost= 10.810412407\n",
      "Epoch: 0005 cost= 9.161843300\n",
      "Epoch: 0006 cost= 7.768702507\n",
      "Epoch: 0007 cost= 7.393570423\n",
      "Epoch: 0008 cost= 15.218988419\n",
      "Epoch: 0009 cost= 9.775375366\n",
      "Epoch: 0010 cost= 5.823899746\n",
      "Epoch: 0011 cost= 8.362657547\n",
      "Epoch: 0012 cost= 4.737264633\n",
      "Epoch: 0013 cost= 10.912469864\n",
      "Epoch: 0014 cost= 7.934610367\n",
      "Epoch: 0015 cost= 11.137799263\n",
      "Epoch: 0016 cost= 7.474731922\n",
      "Epoch: 0017 cost= 12.397913933\n",
      "Epoch: 0018 cost= 6.339158535\n",
      "Epoch: 0019 cost= 5.111407280\n",
      "Epoch: 0020 cost= 7.106061935\n",
      "Epoch: 0021 cost= 3.141166210\n",
      "Epoch: 0022 cost= 5.446547508\n",
      "Epoch: 0023 cost= 6.799573421\n",
      "Epoch: 0024 cost= 8.185979843\n",
      "Epoch: 0025 cost= 2.589447498\n",
      "Epoch: 0026 cost= 2.611497879\n",
      "Epoch: 0027 cost= 2.677704334\n",
      "Epoch: 0028 cost= 4.996290684\n",
      "Epoch: 0029 cost= 2.566218376\n",
      "Epoch: 0030 cost= 2.829747677\n",
      "Optimization Finished!\n",
      "Test data accuracy: 0.287\n",
      "Valid data accuracy: 0.2725\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        offset = (epoch * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_x = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_y = train_labels[offset:(offset + batch_size), :]\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "        # Compute average loss\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Test data accuracy:\", accuracy.eval({x: test_dataset, y: test_labels}))\n",
    "    print(\"Valid data accuracy:\", accuracy.eval({x: valid_dataset, y: valid_labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-beb99eac6533>:66 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Epoch: 0001 cost= 13.494695663\n",
      "Epoch: 1001 cost= 4.388029575\n",
      "Epoch: 2001 cost= 2.001630068\n",
      "Epoch: 3001 cost= 1.129750848\n",
      "Epoch: 4001 cost= 0.982329130\n",
      "Epoch: 5001 cost= 0.702153683\n",
      "Epoch: 6001 cost= 0.747316241\n",
      "Epoch: 7001 cost= 0.834857345\n",
      "Epoch: 8001 cost= 0.677043140\n",
      "Epoch: 9001 cost= 0.681712925\n",
      "Optimization Finished!\n",
      "Test data accuracy: 0.9081\n",
      "Valid data accuracy: 0.8451\n"
     ]
    }
   ],
   "source": [
    "# I just copy the code above\n",
    "# ReLU neuron\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "# param\n",
    "training_epochs = 10000\n",
    "batch_size = 128\n",
    "display_step = 1000\n",
    "n_input = 784 # img shape: 28*28\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# hyper-parameter\n",
    "n_hidden_1 = 1024\n",
    "learning_rate = 0.05\n",
    "lambda_term = 0.01\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # init weights\n",
    "    weights_hiden =  tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=1.0/np.sqrt(n_input)))\n",
    "    weights_out = tf.Variable(tf.random_normal([n_hidden_1, n_classes], stddev=1.0/np.sqrt(n_hidden_1)))\n",
    "    \n",
    "    biases_hidden = tf.Variable(tf.random_normal([n_hidden_1]))\n",
    "    biases_out = tf.Variable(tf.random_normal([n_classes]))\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "    keep = tf.placeholder(tf.float32)\n",
    "    \n",
    "    \n",
    "    def model(x, weights_hiden, weights_out, biases_hidden, biases_out, keep):\n",
    "        # hidden layer with RELU activation\n",
    "        layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights_hiden), biases_hidden))\n",
    "        # apply DropOut to hidden layer\n",
    "        drop_out = tf.nn.dropout(layer_1, keep)\n",
    "        # output layer with linear activation\n",
    "        out_layer = tf.matmul(drop_out, weights_out) + biases_out\n",
    "        return out_layer\n",
    "    \n",
    "    def forward_propagate(x, weights_hiden, weights_out, biases_hidden, biases_out):\n",
    "        # hidden layer with RELU activation\n",
    "        layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights_hiden), biases_hidden))\n",
    "        # output layer with linear activation\n",
    "        out_layer = tf.matmul(layer_1, weights_out) + biases_out\n",
    "        return out_layer\n",
    "    \n",
    "    # Construct model\n",
    "    pred = model(x, weights_hiden, weights_out, biases_hidden, biases_out, keep)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y) +\n",
    "                          lambda_term * tf.nn.l2_loss(weights_hiden) + \n",
    "                          lambda_term * tf.nn.l2_loss(weights_out) +\n",
    "                          lambda_term * tf.nn.l2_loss(biases_hidden) + \n",
    "                          lambda_term * tf.nn.l2_loss(biases_out))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    test_pred = forward_propagate(x, weights_hiden, weights_out, biases_hidden, biases_out)\n",
    "\n",
    "    \n",
    "# run the graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        # Loop over all batches\n",
    "        offset = (epoch * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_x = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_y = train_labels[offset:(offset + batch_size), :]\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y, keep: 0.5})\n",
    "        # Compute average loss\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(test_pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Test data accuracy:\", accuracy.eval({x: test_dataset, y: test_labels}))\n",
    "    print(\"Valid data accuracy:\", accuracy.eval({x: valid_dataset, y: valid_labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Epoch: 0001 cost= 2.331088543  train accuracy:10.94%   test accuracy:30.40%  valid accuracy:28.14% \n",
      "Epoch: 1001 cost= 0.375692695  train accuracy:88.28%   test accuracy:92.35%  valid accuracy:86.29% \n",
      "Epoch: 2001 cost= 0.264163882  train accuracy:90.62%   test accuracy:94.10%  valid accuracy:88.56% \n",
      "Epoch: 3001 cost= 0.213865727  train accuracy:92.97%   test accuracy:94.82%  valid accuracy:89.58% \n",
      "Epoch: 4001 cost= 0.333704442  train accuracy:89.06%   test accuracy:95.16%  valid accuracy:89.86% \n",
      "Epoch: 5001 cost= 0.185625821  train accuracy:92.97%   test accuracy:95.38%  valid accuracy:89.92% \n",
      "Epoch: 6001 cost= 0.282529622  train accuracy:92.19%   test accuracy:95.78%  valid accuracy:90.39% \n",
      "Epoch: 7001 cost= 0.314455748  train accuracy:90.62%   test accuracy:95.71%  valid accuracy:90.56% \n",
      "Epoch: 8001 cost= 0.209148213  train accuracy:95.31%   test accuracy:95.66%  valid accuracy:90.20% \n",
      "Epoch: 9001 cost= 0.178473949  train accuracy:94.53%   test accuracy:95.93%  valid accuracy:90.64% \n",
      "Epoch: 10001 cost= 0.205217302  train accuracy:93.75%   test accuracy:95.97%  valid accuracy:90.89% \n",
      "Epoch: 11001 cost= 0.233908564  train accuracy:92.19%   test accuracy:95.97%  valid accuracy:90.69% \n",
      "Epoch: 12001 cost= 0.114080332  train accuracy:95.31%   test accuracy:95.82%  valid accuracy:90.66% \n",
      "Epoch: 13001 cost= 0.083879113  train accuracy:97.66%   test accuracy:95.79%  valid accuracy:90.86% \n",
      "Epoch: 14001 cost= 0.085020304  train accuracy:98.44%   test accuracy:96.11%  valid accuracy:90.98% \n",
      "Epoch: 15001 cost= 0.190766469  train accuracy:94.53%   test accuracy:95.87%  valid accuracy:90.97% \n",
      "Epoch: 16001 cost= 0.088287726  train accuracy:96.09%   test accuracy:96.00%  valid accuracy:90.81% \n",
      "Epoch: 17001 cost= 0.135608941  train accuracy:95.31%   test accuracy:96.16%  valid accuracy:91.06% \n",
      "Epoch: 18001 cost= 0.158625126  train accuracy:95.31%   test accuracy:96.09%  valid accuracy:90.85% \n",
      "Epoch: 19001 cost= 0.054027461  train accuracy:98.44%   test accuracy:95.94%  valid accuracy:90.74% \n",
      "Epoch: 20001 cost= 0.043073446  train accuracy:99.22%   test accuracy:95.89%  valid accuracy:90.71% \n",
      "Epoch: 21001 cost= 0.065644756  train accuracy:97.66%   test accuracy:96.30%  valid accuracy:91.51% \n",
      "Epoch: 22001 cost= 0.052907638  train accuracy:96.88%   test accuracy:96.22%  valid accuracy:91.09% \n",
      "Epoch: 23001 cost= 0.043250006  train accuracy:98.44%   test accuracy:96.19%  valid accuracy:91.16% \n",
      "Epoch: 24001 cost= 0.097333863  train accuracy:96.09%   test accuracy:96.08%  valid accuracy:91.11% \n",
      "Epoch: 25001 cost= 0.039503369  train accuracy:97.66%   test accuracy:96.26%  valid accuracy:91.18% \n",
      "Epoch: 26001 cost= 0.106948055  train accuracy:98.44%   test accuracy:96.29%  valid accuracy:91.29% \n",
      "Epoch: 27001 cost= 0.131931260  train accuracy:95.31%   test accuracy:96.21%  valid accuracy:91.31% \n",
      "Epoch: 28001 cost= 0.060593035  train accuracy:97.66%   test accuracy:96.28%  valid accuracy:91.43% \n",
      "Epoch: 29001 cost= 0.019534342  train accuracy:99.22%   test accuracy:96.31%  valid accuracy:91.38% \n",
      "Optimization Finished!\n",
      "Test data accuracy:96.40%\n",
      "Valid data accuracy:91.32%\n"
     ]
    }
   ],
   "source": [
    "# improve the accuracy with multi-layer neural network\n",
    "# with l2-regularization and DropOut \n",
    "# Here define 2 hidden-layers Full Connected Neural Network.\n",
    "\n",
    "# I just copy some code above\n",
    "\n",
    "import random\n",
    "\n",
    "# param\n",
    "training_epochs = 30000\n",
    "batch_size = 128\n",
    "display_step = 1000\n",
    "n_input = 784 # img shape: 28*28\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# hyper-parameter\n",
    "n_hidden_1 = 1024\n",
    "n_hidden_2 = 1024\n",
    "n_hidden_3 = 1024\n",
    "learning_rate = 0.005\n",
    "\n",
    "def accuracy_func(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))  / predictions.shape[0])\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # init weights\n",
    "    weights_hidden_1 =  tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=1.0/np.sqrt(n_input)))\n",
    "    weights_hidden_2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=1.0/np.sqrt(n_hidden_1)))\n",
    "    weights_hidden_3 = tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3], stddev=1.0/np.sqrt(n_hidden_2)))    \n",
    "    weights_out = tf.Variable(tf.random_normal([n_hidden_3, n_classes], stddev=1.0/np.sqrt(n_hidden_3)))\n",
    "    # init biases\n",
    "    # biases_hidden_1 = tf.Variable(tf.random_normal([n_hidden_1]))\n",
    "    biases_hidden_1 = tf.Variable(tf.zeros([n_hidden_1]))\n",
    "    # biases_hidden_2 = tf.Variable(tf.random_normal([n_hidden_2]))\n",
    "    biases_hidden_2 = tf.Variable(tf.zeros([n_hidden_2]))\n",
    "    # biases_hidden_3 = tf.Variable(tf.random_normal([n_hidden_3]))\n",
    "    biases_hidden_3 = tf.Variable(tf.zeros([n_hidden_3]))\n",
    "    # biases_out = tf.Variable(tf.random_normal([n_classes]))\n",
    "    biases_out = tf.Variable(tf.zeros([n_classes]))\n",
    "    \n",
    "    # make a dict\n",
    "    weights = {'1': weights_hidden_1, \n",
    "                       '2': weights_hidden_2,  \n",
    "                       '3':weights_hidden_3, \n",
    "                       'out': weights_out}\n",
    "    biases = {'1': biases_hidden_1, \n",
    "                     '2': biases_hidden_2, \n",
    "                     '3':biases_hidden_3, \n",
    "                     'out': biases_out}\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # in fact, this is a forwad propagation function\n",
    "    def forward_propagate(x, weights, bises, keep_prob):\n",
    "        # Hidden layer 1  with RELU activation\n",
    "        layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['1']), biases['1']))\n",
    "        # apply DropOut to hidden layer\n",
    "        drop_out_1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "        # Hidden layer 2 with RELU activation\n",
    "        layer_2 = tf.nn.relu(tf.add(tf.matmul(drop_out_1, weights['2']), biases['2']))\n",
    "        # apply DropOut to hidden layer\n",
    "        drop_out_2 = tf.nn.dropout(layer_2, keep_prob)\n",
    "        # Hidden layer 3 with RELU activation\n",
    "        layer_3 = tf.nn.relu(tf.add(tf.matmul(drop_out_2, weights['3']), biases['3']))\n",
    "        # apply DropOut to hidden layer\n",
    "        drop_out_3 = tf.nn.dropout(layer_3, keep_prob)\n",
    "        # Output layer with linear activation\n",
    "        out_layer = tf.matmul(drop_out_3, weights['out']) + biases['out']\n",
    "        return out_layer\n",
    "\n",
    "\n",
    "    def predict(x, weights_hiden, weights_out, biases_hidden, biases_out):\n",
    "        # Hidden layer 1  with RELU activation\n",
    "        layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['1']), biases['1']))\n",
    "        # Hidden layer 2 with RELU activation\n",
    "        layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['2']), biases['2']))\n",
    "        # Hidden layer 3 with RELU activation\n",
    "        layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['3']), biases['3']))\n",
    "        # Output layer with linear activation\n",
    "        predict = tf.nn.softmax(tf.matmul(layer_3, weights['out']) + biases['out'])\n",
    "        return predict\n",
    "    \n",
    "    # Construct model\n",
    "    logits = forward_propagate(x, weights, biases, keep_prob)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y))\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate=0.25,  global_step=global_step, decay_steps=5000, decay_rate=0.90, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
    "    \n",
    "    pred = tf.nn.softmax(logits)\n",
    "    test_pred = predict(tf.constant(test_dataset), weights_hiden, weights_out, biases_hidden, biases_out)\n",
    "    valid_pred = predict(tf.constant(valid_dataset), weights_hiden, weights_out, biases_hidden, biases_out)\n",
    "\n",
    "\n",
    "# run the graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        offset = (epoch * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_x = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_y = train_labels[offset:(offset + batch_size), :]\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, res_cost, prediction = sess.run([optimizer, cost, pred], feed_dict={x: batch_x, y: batch_y, keep_prob: 0.9})\n",
    "        # Compute average loss\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(res_cost), ' train accuracy:{:.2f}% '.format(accuracy_func(prediction, batch_y)), \n",
    "                  ' test accuracy:{:.2f}%'.format(accuracy_func(test_pred.eval(), test_labels)), \n",
    "                  ' valid accuracy:{:.2f}% '.format(accuracy_func(valid_pred.eval(), valid_labels)))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(test_pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Test data accuracy:{:.2f}%\".format(accuracy_func(test_pred.eval(), test_labels)))\n",
    "    print(\"Valid data accuracy:{:.2f}%\".format(accuracy_func(valid_pred.eval() ,valid_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 1024\n",
    "num_hidden_nodes3 = 1024\n",
    "keep_prob = 0.9\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal(\n",
    "            [image_size * image_size, num_hidden_nodes1], stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal(\n",
    "            [num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    weights3 = tf.Variable(tf.truncated_normal(\n",
    "            [num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "    weights4 = tf.Variable(tf.truncated_normal(\n",
    "            [num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    def forward_propagate():\n",
    "        lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "        drop1 = tf.nn.dropout(lay1_train, keep_prob)\n",
    "        lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "        drop2 = tf.nn.dropout(lay2_train, keep_prob)\n",
    "        lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "        drop3 = tf.nn.dropout(lay3_train, keep_prob)\n",
    "        logits = tf.matmul(drop3, weights4) + biases4\n",
    "        return logits\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    def predict(dataset):\n",
    "        lay1_valid = tf.nn.relu(tf.matmul(dataset, weights1) + biases1)\n",
    "        lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "        lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "        prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "        return prediction\n",
    "\n",
    "    logits = forward_propagate()\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate=0.5, global_step=global_step, decay_steps=5000, decay_rate=0.80, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = predict(tf_valid_dataset)\n",
    "    test_prediction = predict(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-4621452a03a3>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at epoch 0: 2.409556\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 17.0%\n",
      "Minibatch loss at epoch 1000: 0.402116\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at epoch 2000: 0.259861\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at epoch 3000: 0.205248\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at epoch 4000: 0.303189\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at epoch 5000: 0.125371\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at epoch 6000: 0.241323\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at epoch 7000: 0.260685\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at epoch 8000: 0.166088\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at epoch 9000: 0.218168\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at epoch 10000: 0.147686\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at epoch 11000: 0.259209\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at epoch 12000: 0.154597\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at epoch 13000: 0.099061\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at epoch 14000: 0.086198\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at epoch 15000: 0.178661\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at epoch 16000: 0.038974\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at epoch 17000: 0.125212\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at epoch 18000: 0.100344\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at epoch 19000: 0.038207\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at epoch 20000: 0.055157\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at epoch 21000: 0.040461\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at epoch 22000: 0.034941\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at epoch 23000: 0.027663\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at epoch 24000: 0.100975\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at epoch 25000: 0.026030\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at epoch 26000: 0.066157\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at epoch 27000: 0.110057\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at epoch 28000: 0.039223\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at epoch 29000: 0.023513\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at epoch 30000: 0.009996\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.6%\n",
      "Test accuracy: 96.4%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_func(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))  / predictions.shape[0])\n",
    "\n",
    "num_epoch = 30001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for epoch in range(num_epoch):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (epoch * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # feed data\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, c, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (epoch % 1000 == 0):\n",
    "            print(\"Minibatch loss at epoch %d: %f\" % (epoch, c))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy_func(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy_func(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy_func(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
